{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x247863804d0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.io as sio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM_SIZE = 200\n",
    "NUM_NEG_SAMPLES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex = sio.loadmat('data/gold_standard.mat')['gold_standard']['map']\n",
    "world = sio.loadmat('data/world.mat')['world']\n",
    "corpus = sio.loadmat('data/corpus.mat')['corpus']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_world():\n",
    "    \n",
    "    world = sio.loadmat('data/world.mat')['world']\n",
    "    world = world[0][0]\n",
    "    \n",
    "    word_idx = world[0][0] - 1\n",
    "    words = list(itertools.chain.from_iterable(map(list, world[1][0])))  \n",
    "    word_dict = dict(zip(word_idx, words))\n",
    "    \n",
    "    obj_idx = world[3][0] - 1\n",
    "    objs = list(itertools.chain.from_iterable(map(list, world[4][0])))  \n",
    "    obj_dict = dict(zip(obj_idx, objs))\n",
    "    \n",
    "    return word_dict, obj_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gold Standard Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gold_lexicon(word_dict, obj_dict):\n",
    "    \n",
    "    lex = sio.loadmat('data/gold_standard.mat')['gold_standard']['map']\n",
    "    lex = lex[0][0]\n",
    "    \n",
    "    lex_dict_index = {}\n",
    "    lex_dict_readable = {}\n",
    "    for i, j in zip(lex[0], lex[1]):\n",
    "        lex_dict_index[i-1] = j-1\n",
    "        lex_dict_readable[word_dict[i-1]] = obj_dict[j-1]\n",
    "        \n",
    "    return lex_dict_index, lex_dict_readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'a', 1: 'abc', 2: 'about', 3: 'again', 4: 'ah', 5: 'ahhah', 6: 'alison', 7: 'alisons', 8: 'all', 9: 'almost', 10: 'alphabet', 11: 'always', 12: 'ambrose', 13: 'and', 14: 'animals', 15: 'another', 16: 'any', 17: 'anymore', 18: 'are', 19: 'arent', 20: 'around', 21: 'as', 22: 'at', 23: 'aw', 24: 'b', 25: 'baa', 26: 'baby', 27: 'back', 28: 'backs', 29: 'ball', 30: 'balls', 31: 'baseball', 32: 'bathroom', 33: 'be', 34: 'beads', 35: 'beak', 36: 'bear', 37: 'because', 38: 'belongs', 39: 'better', 40: 'big', 41: 'bigbird', 42: 'bigbirds', 43: 'bird', 44: 'birdie', 45: 'bit', 46: 'blue', 47: 'boo', 48: 'book', 49: 'books', 50: 'bottle', 51: 'bottles', 52: 'bowtie', 53: 'boy', 54: 'bracelet', 55: 'bubbles', 56: 'bumble', 57: 'bumblebee', 58: 'bumpba', 59: 'bunnies', 60: 'bunny', 61: 'bunnyrabbit', 62: 'by', 63: 'bzz', 64: 'c', 65: 'can', 66: 'cant', 67: 'carpet', 68: 'chair', 69: 'chew', 70: 'choo', 71: 'color', 72: 'colors', 73: 'come', 74: 'comes', 75: 'could', 76: 'courtney', 77: 'cow', 78: 'cows', 79: 'cute', 80: 'd', 81: 'dada', 82: 'daddy', 83: 'david', 84: 'davids', 85: 'day', 86: 'did', 87: 'didnt', 88: 'different', 89: 'do', 90: 'dododo', 91: 'does', 92: 'doesnt', 93: 'doing', 94: 'dont', 95: 'down', 96: 'drive', 97: 'drool', 98: 'drooling', 99: 'drooly', 100: 'drop', 101: 'drummer', 102: 'duck', 103: 'duckie', 104: 'e', 105: 'eat', 106: 'eating', 107: 'else', 108: 'even', 109: 'everything', 110: 'eyes', 111: 'f', 112: 'fall', 113: 'falling', 114: 'family', 115: 'feet', 116: 'floor', 117: 'foot', 118: 'foots', 119: 'for', 120: 'forest', 121: 'from', 122: 'fun', 123: 'funny', 124: 'g', 125: 'get', 126: 'gets', 127: 'getting', 128: 'girl', 129: 'go', 130: 'goes', 131: 'going', 132: 'good', 133: 'goodness', 134: 'got', 135: 'grab', 136: 'grandma', 137: 'grandmas', 138: 'green', 139: 'grip', 140: 'h', 141: 'had', 142: 'hand', 143: 'handed', 144: 'hands', 145: 'has', 146: 'hat', 147: 'have', 148: 'he', 149: 'head', 150: 'hear', 151: 'her', 152: 'here', 153: 'heres', 154: 'hes', 155: 'hey', 156: 'hi', 157: 'him', 158: 'hiphop', 159: 'his', 160: 'hit', 161: 'hmm', 162: 'hold', 163: 'home', 164: 'horses', 165: 'how', 166: 'hug', 167: 'hugs', 168: 'huh', 169: 'i', 170: 'if', 171: 'in', 172: 'inch', 173: 'into', 174: 'is', 175: 'isnt', 176: 'it', 177: 'its', 178: 'j', 179: 'jumping', 180: 'just', 181: 'k', 182: 'kind', 183: 'kitty', 184: 'kittycat', 185: 'kittycats', 186: 'knock', 187: 'knocked', 188: 'knocking', 189: 'know', 190: 'l', 191: 'lamb', 192: 'lambie', 193: 'last', 194: 'laugh', 195: 'lay', 196: 'learns', 197: 'left', 198: 'let', 199: 'lets', 200: 'like', 201: 'likes', 202: 'little', 203: 'littler', 204: 'look', 205: 'lookee', 206: 'looking', 207: 'looks', 208: 'lot', 209: 'lots', 210: 'love', 211: 'm', 212: 'macdonald', 213: 'made', 214: 'make', 215: 'makes', 216: 'making', 217: 'many', 218: 'maybe', 219: 'me', 220: 'meow', 221: 'messes', 222: 'mhmm', 223: 'mickeymouse', 224: 'might', 225: 'minniemouse', 226: 'mirror', 227: 'mmm', 228: 'mom', 229: 'momma', 230: 'mommy', 231: 'moms', 232: 'moo', 233: 'moocow', 234: 'moocows', 235: 'more', 236: 'mot', 237: 'mother', 238: 'mots', 239: 'mouth', 240: 'much', 241: 'music', 242: 'must', 243: 'my', 244: 'n', 245: 'need', 246: 'never', 247: 'next', 248: 'nice', 249: 'night', 250: 'no', 251: 'noise', 252: 'noisy', 253: 'nose', 254: 'not', 255: 'now', 256: 'o', 257: 'of', 258: 'off', 259: 'oh', 260: 'oink', 261: 'okay', 262: 'old', 263: 'on', 264: 'one', 265: 'ones', 266: 'open', 267: 'or', 268: 'orange', 269: 'other', 270: 'ou', 271: 'our', 272: 'ours', 273: 'out', 274: 'over', 275: 'p', 276: 'peekaboo', 277: 'pig', 278: 'piggie', 279: 'piggies', 280: 'play', 281: 'playing', 282: 'plenty', 283: 'pool', 284: 'pretty', 285: 'probably', 286: 'put', 287: 'q', 288: 'quackquack', 289: 'r', 290: 'rattle', 291: 'reach', 292: 'read', 293: 'ready', 294: 'red', 295: 'right', 296: 'ring', 297: 'rings', 298: 'roll', 299: 'rollie', 300: 'rolypoly', 301: 'round', 302: 's', 303: 'say', 304: 'says', 305: 'see', 306: 'set', 307: 'she', 308: 'sheep', 309: 'should', 310: 'showed', 311: 'silly', 312: 'sing', 313: 'sister', 314: 'sit', 315: 'sitting', 316: 'smilie', 317: 'so', 318: 'soft', 319: 'some', 320: 'someone', 321: 'sometimes', 322: 'song', 323: 'sorts', 324: 'spin', 325: 'spinning', 326: 'splashes', 327: 'squeak', 328: 'squeaked', 329: 'squirm', 330: 'story', 331: 'straight', 332: 'sure', 333: 'sweet', 334: 'sweetie', 335: 't', 336: 'ta', 337: 'take', 338: 'talk', 339: 'talks', 340: 'taste', 341: 'tastes', 342: 'teeth', 343: 'tell', 344: 'telling', 345: 'tells', 346: 'than', 347: 'that', 348: 'thats', 349: 'the', 350: 'them', 351: 'there', 352: 'theres', 353: 'these', 354: 'they', 355: 'things', 356: 'think', 357: 'this', 358: 'those', 359: 'though', 360: 'thought', 361: 'through', 362: 'time', 363: 'tips', 364: 'to', 365: 'today', 366: 'together', 367: 'too', 368: 'took', 369: 'toy', 370: 'toys', 371: 'trade', 372: 'try', 373: 'trying', 374: 'two', 375: 'uh', 376: 'uhhuh', 377: 'uhuh', 378: 'up', 379: 'us', 380: 'usually', 381: 'v', 382: 'w', 383: 'wanna', 384: 'want', 385: 'wanting', 386: 'wants', 387: 'was', 388: 'watch', 389: 'way', 390: 'we', 391: 'wed', 392: 'went', 393: 'wet', 394: 'what', 395: 'whats', 396: 'when', 397: 'where', 398: 'whered', 399: 'wheres', 400: 'which', 401: 'who', 402: 'whos', 403: 'wiggle', 404: 'will', 405: 'with', 406: 'wont', 407: 'wow', 408: 'x', 409: 'y', 410: 'yeah', 411: 'yellow', 412: 'yes', 413: 'you', 414: 'youll', 415: 'your', 416: 'youre', 417: 'yourself', 418: 'z'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({26: 0,\n",
       "  36: 1,\n",
       "  41: 2,\n",
       "  42: 2,\n",
       "  43: 7,\n",
       "  48: 3,\n",
       "  49: 3,\n",
       "  60: 5,\n",
       "  61: 5,\n",
       "  77: 6,\n",
       "  78: 6,\n",
       "  233: 6,\n",
       "  234: 6,\n",
       "  102: 7,\n",
       "  103: 7,\n",
       "  110: 8,\n",
       "  142: 11,\n",
       "  146: 12,\n",
       "  183: 13,\n",
       "  184: 13,\n",
       "  185: 13,\n",
       "  191: 14,\n",
       "  192: 14,\n",
       "  226: 16,\n",
       "  277: 17,\n",
       "  278: 17,\n",
       "  279: 17,\n",
       "  290: 18,\n",
       "  296: 19,\n",
       "  297: 19,\n",
       "  308: 20,\n",
       "  59: 5,\n",
       "  44: 7},\n",
       " {'baby': 'baby',\n",
       "  'bear': 'bear',\n",
       "  'bigbird': 'bird',\n",
       "  'bigbirds': 'bird',\n",
       "  'bird': 'duck',\n",
       "  'book': 'book',\n",
       "  'books': 'book',\n",
       "  'bunny': 'bunny',\n",
       "  'bunnyrabbit': 'bunny',\n",
       "  'cow': 'cow',\n",
       "  'cows': 'cow',\n",
       "  'moocow': 'cow',\n",
       "  'moocows': 'cow',\n",
       "  'duck': 'duck',\n",
       "  'duckie': 'duck',\n",
       "  'eyes': 'eyes',\n",
       "  'hand': 'hand',\n",
       "  'hat': 'hat',\n",
       "  'kitty': 'kitty',\n",
       "  'kittycat': 'kitty',\n",
       "  'kittycats': 'kitty',\n",
       "  'lamb': 'lamb',\n",
       "  'lambie': 'lamb',\n",
       "  'mirror': 'mirror',\n",
       "  'pig': 'pig',\n",
       "  'piggie': 'pig',\n",
       "  'piggies': 'pig',\n",
       "  'rattle': 'rattle',\n",
       "  'ring': 'ring',\n",
       "  'rings': 'ring',\n",
       "  'sheep': 'sheep',\n",
       "  'bunnies': 'bunny',\n",
       "  'birdie': 'duck'})"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dict, obj_dict = get_world()\n",
    "get_gold_lexicon(word_dict, obj_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{27: 1,\n",
       " 37: 2,\n",
       " 42: 3,\n",
       " 43: 3,\n",
       " 44: 8,\n",
       " 49: 4,\n",
       " 50: 4,\n",
       " 61: 6,\n",
       " 62: 6,\n",
       " 78: 7,\n",
       " 79: 7,\n",
       " 234: 7,\n",
       " 235: 7,\n",
       " 103: 8,\n",
       " 104: 8,\n",
       " 111: 9,\n",
       " 143: 12,\n",
       " 147: 13,\n",
       " 184: 14,\n",
       " 185: 14,\n",
       " 186: 14,\n",
       " 192: 15,\n",
       " 193: 15,\n",
       " 227: 17,\n",
       " 278: 18,\n",
       " 279: 18,\n",
       " 280: 18,\n",
       " 291: 19,\n",
       " 297: 20,\n",
       " 298: 20,\n",
       " 309: 21,\n",
       " 60: 6,\n",
       " 45: 8}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus(word_dict, obj_dict):\n",
    "    \n",
    "    corpus = sio.loadmat('data/corpus.mat')['corpus']\n",
    "    corpus = corpus[0]\n",
    "    \n",
    "    corpus_idx = []\n",
    "    corpus_readable = []\n",
    "    for pair in corpus:\n",
    "        \n",
    "        scene = list(pair[0][0]-1)\n",
    "        utt = list(pair[1][0]-1)\n",
    "        new_pair = {'scene': scene, 'utt': utt}\n",
    "        corpus_idx.append(new_pair)\n",
    "        \n",
    "        scene_word = list(map(lambda x: obj_dict[x], scene))\n",
    "        utt_word = list(map(lambda x: word_dict[x], utt))\n",
    "        new_pair_readable = {'scene': scene_word, 'utt': utt_word}\n",
    "        corpus_readable.append(new_pair_readable)\n",
    "    \n",
    "    return corpus_idx, corpus_readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'scene': [3, 2, 18, 9], 'utt': [5, 204, 390, 65, 292, 49, 83]}\n",
      "{'scene': ['book', 'bird', 'rattle', 'face'], 'utt': ['ahhah', 'look', 'we', 'can', 'read', 'books', 'david']}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alignments(pair):\n",
    "    utt, scene = pair['utt'], pair['scene']\n",
    "    combos = list(itertools.product(utt, scene))\n",
    "    return combos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(vocab_len, obj_len):\n",
    "    # Access with torch.LongTensor()\n",
    "    # word_embeddings(torch.LongTensor([0, 1, 2])).shape = (3, 200)\n",
    "    word_embeddings = EmbeddingModeler(vocab_len, DIM_SIZE)\n",
    "    obj_embeddings = EmbeddingModeler(obj_len, DIM_SIZE)\n",
    "    return word_embeddings, obj_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_len, embedding_size):\n",
    "        super(EmbeddingModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_len, embedding_size)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        return embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apl_loss(word_embeddings, object_embeddings, obj_len, combo, scene):\n",
    "    word, obj = combo[0] - 1, combo[1] - 1 # 1 indexing\n",
    "    neg_samples = np.random.choice(np.setdiff1d(np.arange(obj_len) + 1, scene), NUM_NEG_SAMPLES) - 1 #1 indexing \n",
    "    word_embedding = word_embeddings(word)\n",
    "    torch.sum(torch.max())\n",
    "    \n",
    "    return torch.max() #TODO: make this vectorized, so i=nstead of max ocompared to 0, has to be max compares to torch.zeros)\n",
    "\n",
    "def asl_loss(word_embeddings, object_embeddings, vocab_len):\n",
    "    pass\n",
    "\n",
    "def joint_loss(word_embeddings, object_embeddings, obj_len, vocab_len):\n",
    "    pol_loss = apl_loss()\n",
    "    syn_loss = asl_loss()\n",
    "    return torch.add(pol_loss, syn_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  5,  6,  7,  8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,\n",
       "       20, 21])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.setdiff1d(np.arange(obj_len), [4, 9, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 8, 2, 4, 6, 1, 7, 0, 9, 5])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(np.arange(10), 10, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_embeddings(corpus_idx, corpus_readable, vocab_len, obj_len, loss_type):\n",
    "    \n",
    "    word_embeddings, obj_embeddings = create_embeddings(vocab_len, obj_len)\n",
    "    losses = []\n",
    "    for epoch in range(10):\n",
    "        total_loss = 0\n",
    "        for i, pair in enumerate(corpus_idx):\n",
    "            \n",
    "            readable = corpus_readable[i]\n",
    "            print(f\"UTT: {pair['utt']}\")\n",
    "            print(f\"SCENE: {pair['scene']}\")\n",
    "            print(f\"UTT: {readable['utt']}\")\n",
    "            print(f\"SCENE: {readable['scene']}\")\n",
    "            \n",
    "            combos = get_alignments(pair)\n",
    "            for combo in combos:\n",
    "                \n",
    "                if loss_type == 'anti-polysemy':\n",
    "                    loss = apl_loss(word_embeddings, obj_embeddings, obj_len, combo, pair['scene'])\n",
    "                elif loss_type == 'anti-synonymy':\n",
    "                    loss = asl_loss(word_embeddings, obj_embeddings, vocab_len, combo)\n",
    "                else:\n",
    "                    loss = joint_loss(word_embeddings, obj_embeddings, obj_len, vocab_len, combo)\n",
    "\n",
    "            loss.backward()\n",
    "    \n",
    "    return losses\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UTT: [6, 205, 391, 66, 293, 50, 84]\n",
      "SCENE: [4, 3, 19, 10]\n",
      "UTT: ['ahhah', 'look', 'we', 'can', 'read', 'books', 'david']\n",
      "SCENE: ['book', 'bird', 'rattle', 'face']\n",
      "(6, 4)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "max() received an invalid combination of arguments - got (), but expected one of:\n * (Tensor input)\n * (Tensor input, Tensor other, Tensor out)\n * (Tensor input, int dim, bool keepdim, tuple of Tensors out)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-84-998dd452cca1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_readable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'anti-polysemy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-83-3cdacd1a4bc3>\u001b[0m in \u001b[0;36mtrain_embeddings\u001b[1;34m(corpus_idx, corpus_readable, vocab_len, obj_len, loss_type)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mloss_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'anti-polysemy'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m                     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapl_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj_embeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcombo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[0mloss_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'anti-synonymy'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0masl_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj_embeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcombo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-80-daf81594bdac>\u001b[0m in \u001b[0;36mapl_loss\u001b[1;34m(word_embeddings, object_embeddings, obj_len, combo)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcombo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#TODO: make this vectorized, so i=nstead of max ocompared to 0, has to be max compares to torch.zeros)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0masl_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobject_embeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: max() received an invalid combination of arguments - got (), but expected one of:\n * (Tensor input)\n * (Tensor input, Tensor other, Tensor out)\n * (Tensor input, int dim, bool keepdim, tuple of Tensors out)\n"
     ]
    }
   ],
   "source": [
    "train_embeddings(corpus_idx, corpus_readable, vocab_len, obj_len, 'anti-polysemy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict, obj_dict = get_world()\n",
    "gold_idx, gold_readable = get_gold_lexicon(word_dict, obj_dict)\n",
    "corpus_idx, corpus_readable = get_corpus(word_dict, obj_dict)\n",
    "\n",
    "vocab_len = len(word_dict)\n",
    "obj_len = len(obj_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['When', 'forty'], 'winters'), (['forty', 'winters'], 'shall'), (['winters', 'shall'], 'besiege')]\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10\n",
    "# We will use Shakespeare Sonnet 2\n",
    "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\".split()\n",
    "# we should tokenize the input, but we will ignore that for now\n",
    "# build a list of tuples.  Each tuple is ([ word_i-2, word_i-1 ], target word)\n",
    "trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2])\n",
    "            for i in range(len(test_sentence) - 2)]\n",
    "# print the first 3, just so you can see what they look like\n",
    "print(trigrams[:3])\n",
    "\n",
    "vocab = set(test_sentence)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "\n",
    "class NGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "#         print(self.embeddings.weight.shape)\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "#         print(embeds.shape)\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "\n",
    "\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[515.5346808433533, 513.2982707023621, 511.07346773147583, 508.8592507839203, 506.6559977531433, 504.46337628364563, 502.276953458786, 500.094162940979, 497.91767406463623, 495.74654150009155, 493.5804042816162]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for context, target in trigrams:\n",
    "\n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in tensors)\n",
    "        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
    "#         print(context_idxs)\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        log_probs = model(context_idxs)\n",
    "#         print(log_probs.shape)\n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a tensor)\n",
    "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
    "\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)\n",
    "print(losses)  # The los"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
